{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpDNbMfiMEU6",
        "outputId": "1d1571d1-8ab6-4b44-8b56-6fa90c332ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8fgHymwXFkQ"
      },
      "source": [
        "<hr color=darkblue>\n",
        "\n",
        "<strong><font color=darkblue size=6>Task 1: Theory</font></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7JMHUlyQ-yP"
      },
      "source": [
        "## **Task 1.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1TRMidDY_Tp"
      },
      "source": [
        "Consider $K$ classes of normally distributed real-valued random variables with shared standard deviation, $\\sigma$, but class-specific means, $\\mu_k$. Show that if class $k$ maximizes the discriminant function, $\\delta_k$, it then also maximizes the posterior for class $k$, i.e. $p_k$ defined in ISLR4.4.1. Rigorously justify each transition. FYI: this is a one-directional claim (mostly algebraic manipulations). *Hint*: carefully review section ISLR4.4 and the relation between the posterior and the corresponding discriminant function. Start by formulaically defining all components, including the posterior and discriminant function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrMKoLDSO5g0"
      },
      "source": [
        "## **SOLUTION**\n",
        "\n",
        "We are given $K$ classes of Gaussian random variables, so that for the $k^{th}$ class: $X_{i,k}\\sim\\mathcal{N}(\\mu_k, \\sigma_k^2)$, so that $\\mu_k$ is class-specific, but $\\forall i, j:\\sigma_i = \\sigma_j$.\n",
        "\n",
        "From ISLR book we know that the discriminant function for an abservation instance $x$ of the $X$ RV has the follwing look ($\\pi_k$ indicates the prior probability for $x$ to belong to $k^{th}$ class):\n",
        "$$\n",
        "\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log{\\pi_k}.\\tag{1}\n",
        "$$\n",
        "Whilst the posterior probability of $x$ has the follwing definition:\n",
        "$$\n",
        "p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\right)}{\\sum_{l=1}^{K}\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\right)}\\tag{2}\n",
        "$$\n",
        "\n",
        "From ISLR, we know, that an observation $X=x$ is assigned to the very class, for which $p_k(x)$ is largest, same applies, once $\\delta_k(x)$ is largest for given observation instance $x$. We now have to show such a relation explicitly, so that if some $\\tilde k$ maximizes $\\delta(x)$, then $p(x)$ is maximized as well. Let us rewrite $(1)$ as follows:\n",
        "\n",
        "$$\n",
        "\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log{\\pi_k} = \\frac{\\mu_k(2x-\\mu_k)}{2\\sigma^2} + \\log{\\pi_k} = \\frac{-(x-\\mu_k)^2 + x^2}{2\\sigma^2} + \\log\\pi_k = (\\text{by properties of log})=\\\\=\\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(\\frac{-(x-\\mu_k)^2}{2\\sigma^2}\\right)\\right) + \\log\\exp\\left(\\frac{x^2}{2\\sigma^2}\\right) - \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right) + \\log\\pi_k = \\log\\left(\\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\right)}{\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)}\\right).\n",
        "$$\n",
        "\n",
        "Here we can see that $\\exp\\delta_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\right)}{\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)}$, and now: $\\underset{k}{\\text{argmax}~}\\delta_k(x) =\\tilde{k} \\Rightarrow \\underset{k}{\\text{argmax}~}\\exp\\delta_k(x)=\\tilde{k}$, for which the numerator will be maximized, in order to obtain the largest-possible fraction, so that by maximizing $\\exp\\delta_k(x)$ over $k$, we are actually maximizing the numerator of $\\exp{\\delta_k(x)}$ (as denominator does not depend on $k$, and we cannot minimize it), hence $\\underset{k}{\\text{argmax}~}\\delta_k(x) =\\tilde{k} \\Rightarrow \\underset{k}{\\text{argmax}~}\\exp\\delta_k(x)=\\tilde{k} \\Rightarrow \\underset{k}{\\text{argmax}~}N(x, \\pi_k, \\mu_k, \\sigma) = \\tilde k$, where: $$N(x, \\pi_k, \\mu_k, \\sigma) = \\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\right).$$\n",
        "\n",
        "Now, let us note, that numerator of $p_k(x)$ is in fact $N(x,\\pi_k, \\mu_k,\\sigma)$, and hence as by maximizing $N(x,\\pi_k, \\mu_k,\\sigma)$, we are maximizing the numerator of $\\pi_k(x)$, what automatically implies maximizing the whole $\\pi_k(x)$ over $k$ itself, of course $N(x,\\pi_k, \\mu_k,\\sigma)$ is also included in the denominator, but we are still looking for the best $k$ to maximize the numerator, as far as denominator actually accounts for all possible classes $l \\in [K]$ so that it can be ignored in maximization problem of $p_k(x)$, since it (denominator) is going to stay fixed for any chosen $k$. So that we finally obtain the following maximization sequence: $$\\underset{k}{\\text{argmax}~}\\delta_k(x) =\\tilde{k} \\Rightarrow \\underset{k}{\\text{argmax}~}\\exp\\delta_k(x)=\\tilde{k} \\Rightarrow \\underset{k}{\\text{argmax}~}N(x, \\pi_k, \\mu_k, \\sigma)~[\\text{numerator of }p_k(x)]= \\tilde k \\Rightarrow\\underset{k}{\\text{argmax}~}p_k(x)=\\tilde k.~\\blacksquare$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNy1mpSBRCzp"
      },
      "source": [
        "## **Task 1.2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Mefo4AQ2g7"
      },
      "source": [
        "Given  [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) random variables $X_1,...,X_n$, each of the form $X\\sim\\text{Gamma}(2, \\theta)$ with [PDF](https://en.wikipedia.org/wiki/Probability_density_function):\n",
        "\n",
        "$$f\\left(X \\mid \\theta\\right)=\\theta^{2} \\cdot X \\cdot e ^{-X \\theta}, $$\n",
        "\n",
        "where $ \\quad X>0 , \\quad \\theta>0, \\quad \\mathbb E\\left(X\\right)= \\frac{2}{\\theta}$\n",
        "\n",
        "**(a)** Assign the $\\text{Gamma}(\\alpha, \\beta)$ prior to $\\theta$ and derive an expression of the corresponding [posterior PDF](https://en.wikipedia.org/wiki/Posterior_probability).\n",
        "\n",
        "**(b)** Find a [Bayes estimator](https://en.wikipedia.org/wiki/Bayes_estimator) for $\\theta$ based on the posterior in part (a).\n",
        "\n",
        "**(c)** Discuss how you can calculate a 95% [credible interval](https://en.wikipedia.org/wiki/Credible_interval). You do not need to calculate it. Provide the interpretation of such an interval. \n",
        "\n",
        "**(d)** Assume that $n = 5$ and $\\sum_{i=1}^{n} X_{i}=29.6$. Find the [Bayes factor](https://en.wikipedia.org/wiki/Bayes_factor) for the hypotheses $H_0: \\theta = 0.5$ and $H_1: \\theta = 1$ and provide an interpretation of it. Note that you can answer in terms of logarithms or exponentials and there is no need to give the final number.\n",
        "\n",
        "**Remark**: you can refresh Bayesian statistics by reviewing [*Computer Age Statistical Inference, Ch.3, p.22*](https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf#page=40) or [Stanford's STAT200 Lecture Notes](https://web.stanford.edu/class/stats200/Lecture20.pdf). Recommended videos: [🎦](https://www.youtube.com/results?search_query=bayesian+inference), [&#127910;](https://www.youtube.com/results?search_query=Bayes+estimator), [&#127910;](https://www.youtube.com/results?search_query=credible+interval), [&#127910;](https://www.youtube.com/results?search_query=Bayes+factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWqv112-X8cC"
      },
      "source": [
        "## **SOLUTION**\n",
        "**(a)** The posterior PDF for $(\\theta, x)$ has the follwing form: $p(\\theta|x)\\propto p(x|\\theta)p(\\theta)$, where $p(\\theta)$ is a prior PDF, and $p(x|\\theta)$ is the Likelihood function of $x$.\n",
        "\n",
        "Now, according to the definition of the Likelihood function:\n",
        "$$\n",
        "p(x|\\theta) = \\prod^n_{i=1} f(X_i|\\theta) = \\prod^n_{i=1}\\theta^2\\cdot X_i \\cdot e^{-X_i \\theta} = \\theta^{2n} \\cdot \\exp\\left(-\\theta\\sum\\limits^n_{i=1}X_i \\right) \\cdot \\prod^n_{i=1}X_i.\n",
        "$$\n",
        "Now, we obtain:\n",
        "$$p(\\theta|x)\\propto p(x|\\theta)p(\\theta)~ \\underset{\\text{Gamma}(\\alpha, \\beta)}{\\propto}~ p(x|\\theta)\\cdot \\theta^{\\alpha-1}e^{-\\beta\\theta} \\propto \\theta^{2n}e^{-\\theta\\sum\\limits^n_{i=1}X_i}\\theta^{\\alpha-1}e^{-\\beta\\theta} = \\theta^{2n+\\alpha-1}e^{-\\theta(\\beta + \\sum\\limits^n_{i=1}X_i)}~\\text{what yields }p(\\theta|x) \\text{ to follow the distribution of: Gamma}(2n+\\alpha, \\beta+\\sum\\limits^n_{i=1}X_i).$$\n",
        "\n",
        "**(b)** The Bayes estimator for $(x, \\theta)$, according to the posterior distribution, would be:\n",
        "$$\n",
        "\\mathbb{E}[\\theta|X] = \\frac{2n+\\alpha}{\\beta+\\sum\\limits^n_{i=1}X_i}.\n",
        "$$\n",
        "\n",
        "**(c)** The $0.95$ credible interval ($\\alpha = 0.05$) is formed by the interval from the $\\frac{\\alpha}{2}$ to $1- \\frac{\\alpha}{2}$ respective quartiles from the posterior distribution of $\\text{Gamma}(2n+\\alpha, \\beta+\\sum\\limits^n_{i=1}X_i)$.\n",
        "\n",
        "**(d)** According to the given conditions, we can compute the Bayes factor to be:\n",
        "$$\n",
        "B(\\theta)_{H_{0,1}} = \\frac{\\text{Ga}(10 + \\alpha, \\beta + 29.9)[\\theta=0.5]/\\text{Ga}(\\alpha, \\beta)[\\theta=0.5]}{\\text{Ga}(10 + \\alpha, \\beta + 29.9)[\\theta=1]/\\text{Ga}(\\alpha, \\beta)[\\theta=1]}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kNhWcb3ZC2O"
      },
      "source": [
        "<hr color=darkblue>\n",
        "\n",
        "<strong><font color=darkblue size=6>Task 2: Kaggle-HAR </font></strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ8wCtt3audM"
      },
      "source": [
        "\n",
        "**Private URL** for students (allows submission) is in Moodle's HW assignment. **Public URL** with read-only access is [here](https://www.kaggle.com/c/hse-ml-hw11-nov-22-har/rules). See competition rules, submission, grading, dataset, and performance metric. The **starter code** below produces a baseline model, which you should beat, while respecting the competition rules. Your code starts after the timer. This is your baseline model. Remember to seed [RNG](https://en.wikipedia.org/wiki/Random_number_generation) in all experiments for reproducibility.\n",
        "\n",
        "**Instructions for enabling Kaggle API in Colab**:\n",
        "1. Accept competition rules before running [Kaggle API](https://github.com/Kaggle/kaggle-api#api-credentials). [Loading Kaggle dataset example](https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab)\n",
        "1. In your Kaggle Account, [Create API Token](https://github.com/Kaggle/kaggle-api#api-credentials) and save the resulting **kaggle.json** file to the [root of your Google Drive](https://drive.google.com/drive/u/0/my-drive) \n",
        "2. In Colab, open **Files** panel 🗀 (on the left) and click gray folder icon <font color=gray>🖿</font> to mount your Google drive\n",
        "\n",
        "Your Kaggle/Google Drive credentials are secure; and Colab's kaggle.json only lasts a Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A5ccIwGNPgn",
        "outputId": "8df99c29-4f5e-4002-8b5f-f470a27a9fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: hse-ml-hw11-nov-22-har\n",
            "100% 601M/601M [00:06<00:00, 94.8MB/s]\n",
            "Using competition: hse-ml-hw11-nov-22-har\n",
            " teamId  teamName                                            submissionDate       score    \n",
            "-------  --------------------------------------------------  -------------------  -------  \n",
            "7807185  Jingtao Xu                                          2021-12-05 19:34:59  0.96877  \n",
            "7806289  IB-My Little Pony-Adshead,Ivanushkina,Odnakov       2021-12-01 21:15:22  0.96673  \n",
            "7816033  DJ-Fancy Kaggle group name-Vasyutin,Kolobaev,Veips  2021-12-05 18:57:11  0.96469  \n",
            "7822077  Arina Fayzulina                                     2021-12-05 18:12:07  0.96469  \n",
            "7834510  KagDm                                               2021-12-05 12:27:31  0.96401  \n",
            "7822288  Kseniya Yakunina                                    2021-12-04 19:25:21  0.96266  \n",
            "7815606  IA-Red Eagles-Argunov,Lukianov,Miniakhmetov         2021-12-04 20:56:27  0.96130  \n",
            "7839294  DI-DeadInside-Ivanov,Mozharov,Pershina              2021-12-05 18:43:33  0.95926  \n",
            "7815072  IvanushkinaEkaterina                                2021-12-01 21:38:18  0.95858  \n",
            "7816048  Ushakov Stanislav                                   2021-12-03 12:08:59  0.95790  \n",
            "7838810  DQ-Stressed Out-Korzun,Parakal                      2021-12-05 17:21:08  0.95790  \n",
            "7831117  OA-Kam-Zyalaletdinova                               2021-12-05 19:47:03  0.95790  \n",
            "7813466  DV-RedBuy-Ignatev,Rutsinskiy,Kurbatov               2021-12-05 17:18:48  0.95723  \n",
            "7820227  DG-DolceGabanna-Vasiliev, Ivanov, Karcha            2021-12-05 14:00:09  0.95723  \n",
            "7839235  Polya Pers                                          2021-12-05 15:09:22  0.95723  \n",
            "7834300  DP-TeamName-Dryagin,Sharara,Zotov                   2021-12-05 18:10:18  0.95451  \n",
            "7838297  really deep                                         2021-12-05 14:03:28  0.95247  \n",
            "7829458  Oleg Malchenko                                      2021-12-05 19:01:03  0.94908  \n",
            "7840148  Denis Cherepanov                                    2021-12-05 19:58:14  0.94840  \n",
            "7825026  Igor Poletaev                                       2021-12-03 14:23:31  0.94704  \n",
            "7815955  Maria Adshead                                       2021-11-30 19:45:45  0.94568  \n",
            "7831170  Arhiburya                                           2021-12-03 17:24:47  0.94501  \n",
            "7809114  ScrewCameras                                        2021-12-05 19:14:48  0.94501  \n",
            "7830670  DL-Masters-Baminiwatte,Gyursoy,Repin                2021-12-05 15:55:19  0.94433  \n",
            "7839883  teamteam                                            2021-12-05 19:41:23  0.94433  \n",
            "7767479  HAR_baseline.csv                                    2021-11-22 05:48:49  0.92939  \n",
            "7838607  Danila Kokin                                        2021-12-05 15:58:28  0.92464  \n",
            "7825956  Pavel Calmanovici                                   2021-12-05 16:22:06  0.61235  \n"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                                           # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log                   # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                              # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v hse-ml-hw11-nov-22-har   # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log                          # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                                        # Kaggle dataset is copied as a single file and needs to be unzipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yRDzbPONXQ0",
        "outputId": "c8afd678-cabc-40c7-bcd3-9fd7d55dd215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2.81 s, sys: 402 ms, total: 3.21 s\n",
            "Wall time: 21.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "!pip -q install -U plotly > log\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\" \n",
        "import numpy as np, pandas as pd, time, matplotlib.pyplot as plt, os, plotly.express as px\n",
        "import tensorflow as tf, tensorflow.keras as keras\n",
        "from sklearn.neural_network import MLPClassifier   # SKLearn's MLP is optimised for CPU (and doesn't use GPU)\n",
        "from keras.layers import Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.set_printoptions(linewidth=10000, precision=2, edgeitems=20, suppress=True)\n",
        "pd.set_option('max_colwidth', 1000, 'max_columns', 100, 'display.width', 1000, 'max_rows', 4)\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'⏳ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "CB84KHwAVfRF",
        "outputId": "26ceb6f4-03f3-4d60-93ea-74992320ff50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 276 ms, sys: 37.7 ms, total: 313 ms\n",
            "Wall time: 319 ms\n",
            "CPU times: user 41.4 s, sys: 7.45 s, total: 48.9 s\n",
            "Wall time: 48.8 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>...</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "      <th>520</th>\n",
              "      <th>521</th>\n",
              "      <th>522</th>\n",
              "      <th>523</th>\n",
              "      <th>524</th>\n",
              "      <th>525</th>\n",
              "      <th>526</th>\n",
              "      <th>527</th>\n",
              "      <th>528</th>\n",
              "      <th>529</th>\n",
              "      <th>530</th>\n",
              "      <th>531</th>\n",
              "      <th>532</th>\n",
              "      <th>533</th>\n",
              "      <th>534</th>\n",
              "      <th>535</th>\n",
              "      <th>536</th>\n",
              "      <th>537</th>\n",
              "      <th>538</th>\n",
              "      <th>539</th>\n",
              "      <th>540</th>\n",
              "      <th>541</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2778</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0676</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9390</td>\n",
              "      <td>-0.4230</td>\n",
              "      <td>-0.7520</td>\n",
              "      <td>0.8496</td>\n",
              "      <td>0.6226</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-1.0370</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-1.0070</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-0.6772</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.5900</td>\n",
              "      <td>-0.3162</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.4440</td>\n",
              "      <td>-0.2622</td>\n",
              "      <td>0.1092</td>\n",
              "      <td>0.4468</td>\n",
              "      <td>-0.4443</td>\n",
              "      <td>-0.1484</td>\n",
              "      <td>0.1718</td>\n",
              "      <td>-0.2727</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>-0.4720</td>\n",
              "      <td>-0.5264</td>\n",
              "      <td>0.2332</td>\n",
              "      <td>0.9640</td>\n",
              "      <td>-0.1309</td>\n",
              "      <td>0.1071</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9126</td>\n",
              "      <td>-0.2037</td>\n",
              "      <td>-0.5300</td>\n",
              "      <td>-0.8164</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.8850</td>\n",
              "      <td>-0.9033</td>\n",
              "      <td>-0.9120</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-1.014</td>\n",
              "      <td>-0.9560</td>\n",
              "      <td>-0.6780</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.6180</td>\n",
              "      <td>-0.1021</td>\n",
              "      <td>-0.5977</td>\n",
              "      <td>-0.9546</td>\n",
              "      <td>-0.9110</td>\n",
              "      <td>-0.9260</td>\n",
              "      <td>-0.9297</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.9460</td>\n",
              "      <td>-1.022</td>\n",
              "      <td>-0.9570</td>\n",
              "      <td>-0.2930</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.3455</td>\n",
              "      <td>-0.1411</td>\n",
              "      <td>-0.5215</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9890</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.4453</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>-0.5415</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.5093</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>-0.0912</td>\n",
              "      <td>-0.1415</td>\n",
              "      <td>-0.1316</td>\n",
              "      <td>-0.8200</td>\n",
              "      <td>0.1721</td>\n",
              "      <td>-0.0535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>-0.1046</td>\n",
              "      <td>-0.2010</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>-0.2776</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>-0.2605</td>\n",
              "      <td>-0.0572</td>\n",
              "      <td>-0.0364</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.1448</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>-0.0844</td>\n",
              "      <td>-0.6733</td>\n",
              "      <td>-0.7603</td>\n",
              "      <td>-0.7847</td>\n",
              "      <td>-0.4136</td>\n",
              "      <td>-0.3633</td>\n",
              "      <td>-0.1837</td>\n",
              "      <td>0.2830</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>-0.2502</td>\n",
              "      <td>0.3079</td>\n",
              "      <td>-0.1384</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.1969</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>-0.3472</td>\n",
              "      <td>-0.1434</td>\n",
              "      <td>-0.4058</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.9326</td>\n",
              "      <td>-0.2942</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.9634</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8115</td>\n",
              "      <td>0.4165</td>\n",
              "      <td>-0.4731</td>\n",
              "      <td>-0.8210</td>\n",
              "      <td>0.2542</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.2688</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>-0.7710</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.1018</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>-0.8994</td>\n",
              "      <td>-0.0642</td>\n",
              "      <td>-0.0842</td>\n",
              "      <td>-0.4750</td>\n",
              "      <td>-0.1345</td>\n",
              "      <td>-0.3853</td>\n",
              "      <td>-0.2573</td>\n",
              "      <td>-0.5430</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.1365</td>\n",
              "      <td>-0.677</td>\n",
              "      <td>-0.1826</td>\n",
              "      <td>0.6777</td>\n",
              "      <td>-0.7866</td>\n",
              "      <td>0.3240</td>\n",
              "      <td>-0.6206</td>\n",
              "      <td>-0.8530</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.3025</td>\n",
              "      <td>-0.3176</td>\n",
              "      <td>-0.3198</td>\n",
              "      <td>-0.6426</td>\n",
              "      <td>-0.2488</td>\n",
              "      <td>-0.7236</td>\n",
              "      <td>-0.2512</td>\n",
              "      <td>0.6177</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>-0.0397</td>\n",
              "      <td>-0.4220</td>\n",
              "      <td>0.5480</td>\n",
              "      <td>0.6455</td>\n",
              "      <td>0.2296</td>\n",
              "      <td>-0.0335</td>\n",
              "      <td>-0.7000</td>\n",
              "      <td>0.2998</td>\n",
              "      <td>0.0880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>-0.0132</td>\n",
              "      <td>-0.1257</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9346</td>\n",
              "      <td>-0.5630</td>\n",
              "      <td>-0.8394</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6846</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0160</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.6500</td>\n",
              "      <td>-0.5225</td>\n",
              "      <td>-0.7974</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.2532</td>\n",
              "      <td>0.3723</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>-0.2756</td>\n",
              "      <td>0.3179</td>\n",
              "      <td>-0.1398</td>\n",
              "      <td>0.0948</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>-0.1853</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>-0.0402</td>\n",
              "      <td>-0.0880</td>\n",
              "      <td>0.9785</td>\n",
              "      <td>-0.0442</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.028</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9697</td>\n",
              "      <td>-1.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2356</td>\n",
              "      <td>0.4312</td>\n",
              "      <td>-0.6030</td>\n",
              "      <td>-0.8706</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>-1.0210</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>-0.4620</td>\n",
              "      <td>-0.7485</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-1.008</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.8574</td>\n",
              "      <td>-0.9210</td>\n",
              "      <td>0.1049</td>\n",
              "      <td>-0.6284</td>\n",
              "      <td>-0.8970</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0030</td>\n",
              "      <td>-0.844</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>-0.3782</td>\n",
              "      <td>-0.7183</td>\n",
              "      <td>-0.0227</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.1864</td>\n",
              "      <td>0.4556</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.0595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2695</td>\n",
              "      <td>-0.0251</td>\n",
              "      <td>-0.1010</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9050</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.8920</td>\n",
              "      <td>-0.9673</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.5293</td>\n",
              "      <td>-0.8022</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.6714</td>\n",
              "      <td>0.8480</td>\n",
              "      <td>-0.9624</td>\n",
              "      <td>-1.0205</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9600</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9480</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.7320</td>\n",
              "      <td>-0.5117</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0.3710</td>\n",
              "      <td>-0.2270</td>\n",
              "      <td>0.2700</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>-0.2438</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>-0.2886</td>\n",
              "      <td>-0.3882</td>\n",
              "      <td>0.6284</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>-0.1277</td>\n",
              "      <td>0.0722</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.925</td>\n",
              "      <td>-0.9440</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>0.0488</td>\n",
              "      <td>-0.3591</td>\n",
              "      <td>-0.7050</td>\n",
              "      <td>-1.0240</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.013</td>\n",
              "      <td>-0.9860</td>\n",
              "      <td>-0.9650</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.1430</td>\n",
              "      <td>-0.1555</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>-0.9320</td>\n",
              "      <td>-0.9200</td>\n",
              "      <td>-0.9424</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.932</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.985</td>\n",
              "      <td>-0.9463</td>\n",
              "      <td>-0.4020</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.3160</td>\n",
              "      <td>-0.0948</td>\n",
              "      <td>-0.4695</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>-0.9976</td>\n",
              "      <td>-0.9680</td>\n",
              "      <td>-1.0340</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.6980</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.4863</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>-0.3293</td>\n",
              "      <td>-0.0127</td>\n",
              "      <td>-0.1399</td>\n",
              "      <td>0.4624</td>\n",
              "      <td>-0.7610</td>\n",
              "      <td>-0.8696</td>\n",
              "      <td>0.1720</td>\n",
              "      <td>-0.0272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows × 562 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        y       0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43     44      45      46      47      48  ...     511     512     513     514     515     516     517     518     519     520    521     522     523     524     525     526     527     528     529     530     531    532     533    534     535     536     537     538     539     540     541     542     543     544     545     546     547     548     549    550     551     552     553     554     555     556     557     558     559     560\n",
              "0       5  0.2778  0.0092 -0.0676 -0.9785 -0.9160 -0.9610 -0.9834 -0.9170 -0.9590 -0.9390 -0.4230 -0.7520  0.8496  0.6226  0.8400 -0.9434 -0.9614 -1.0370 -1.0150 -1.0070 -0.9640 -0.9550 -0.6772  0.0568  0.0192  0.5900 -0.3162  0.1833  0.4440 -0.2622  0.1092  0.4468 -0.4443 -0.1484  0.1718 -0.2727  0.0954 -0.4720 -0.5264  0.2332  0.9640 -0.1309  0.1071 -0.9814 -0.948 -0.9727 -0.9720 -0.9575 -0.9585  ... -0.9126 -0.2037 -0.5300 -0.8164 -0.9170 -0.8850 -0.9033 -0.9120 -0.9750 -0.9326 -1.014 -0.9560 -0.6780 -0.9966 -0.6180 -0.1021 -0.5977 -0.9546 -0.9110 -0.9260 -0.9297 -1.017 -0.9460 -1.022 -0.9570 -0.2930 -1.0100 -0.3455 -0.1411 -0.5215 -0.9585 -0.9160 -0.9434 -0.9414 -0.9750 -0.9414 -0.9890 -0.9610 -0.4453 -1.002 -0.5415 -0.0308 -0.5093  0.0380 -0.0912 -0.1415 -0.1316 -0.8200  0.1721 -0.0535\n",
              "1       1  0.2454  0.0073 -0.1046 -0.2010  0.1426 -0.2668 -0.2776  0.0648 -0.2605 -0.0572 -0.0364 -0.2830 -0.2830 -0.1448  0.4443 -0.0844 -0.6733 -0.7603 -0.7847 -0.4136 -0.3633 -0.1837  0.2830  0.5100  0.0582 -0.2502  0.3079 -0.1384  0.0822  0.0902 -0.0034  0.1969  0.0538  0.2996 -0.0258  0.0936 -0.3472 -0.1434 -0.4058  0.3690  0.9326 -0.2942 -0.0916 -0.9966 -0.964 -0.9663 -0.9746 -0.9736 -0.9634  ... -0.8115  0.4165 -0.4731 -0.8210  0.2542  0.2410  0.2688  0.0928 -0.7710  0.2430 -0.221 -0.1018  0.7134 -0.8994 -0.0642 -0.0842 -0.4750 -0.1345 -0.3853 -0.2573 -0.5430 -0.757 -0.1365 -0.677 -0.1826  0.6777 -0.7866  0.3240 -0.6206 -0.8530 -0.2500 -0.3025 -0.3176 -0.3198 -0.6426 -0.2488 -0.7236 -0.2512  0.6177 -0.910  0.1069 -0.0397 -0.4220  0.5480  0.6455  0.2296 -0.0335 -0.7000  0.2998  0.0880\n",
              "...    ..     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
              "499998  4  0.2740 -0.0132 -0.1257 -0.9834 -1.0020 -0.9590 -0.9897 -0.9746 -0.9873 -0.9346 -0.5630 -0.8394  0.8306  0.6846  0.8350 -0.9840 -0.9824 -0.9960 -1.0200 -0.9950 -1.0160 -0.9590 -0.6500 -0.5225 -0.7974  0.5020 -0.2532  0.3723  0.1772  0.2920 -0.2756  0.3179 -0.1398  0.0948  0.0180 -0.1853  0.1871  0.0790 -0.0402 -0.0880  0.9785 -0.0442 -0.0532 -0.9950 -1.028 -0.9790 -1.0010 -0.9697 -1.0060  ... -0.2356  0.4312 -0.6030 -0.8706 -0.9700 -0.9863 -1.0010 -0.9990 -0.9927 -0.9910 -1.020 -1.0210 -0.9920 -0.9736  0.3857 -0.4620 -0.7485 -0.9985 -0.9575 -0.9897 -0.9814 -1.000 -0.9990 -1.008 -0.9660 -0.8574 -0.9210  0.1049 -0.6284 -0.8970 -1.0200 -1.0150 -0.9750 -1.0170 -0.9746 -0.9937 -0.9927 -0.9950 -1.0030 -0.844  0.2454 -0.3782 -0.7183 -0.0227  0.1957  0.1864  0.4556 -0.9326  0.1137  0.0595\n",
              "499999  5  0.2695 -0.0251 -0.1010 -1.0170 -0.9050 -0.9375 -0.9736 -0.8920 -0.9673 -0.9575 -0.5293 -0.8022  0.8530  0.6714  0.8480 -0.9624 -1.0205 -0.9900 -0.9600 -0.9960 -0.9480 -0.9720 -0.7320 -0.5117 -0.3535  0.3710 -0.2270  0.2700 -0.0636 -0.2438  0.0608  0.2050 -0.0218 -0.1199  0.0678  0.0154 -0.1132 -0.2886 -0.3882  0.6284  0.9966 -0.1277  0.0722 -1.0050 -0.925 -0.9440 -1.0050 -0.9824 -0.9233  ... -0.9500  0.0488 -0.3591 -0.7050 -1.0240 -0.9790 -0.9746 -0.9814 -0.9920 -0.9814 -1.013 -0.9860 -0.9650 -1.0150 -0.1430 -0.1555 -0.5180 -0.9320 -0.9200 -0.9424 -0.9326 -0.932 -0.9170 -0.985 -0.9463 -0.4020 -0.9640 -0.3160 -0.0948 -0.4695 -0.9590 -0.9500 -0.9976 -0.9680 -1.0340 -0.9727 -0.9900 -0.9790 -0.6980 -1.017 -0.4863  0.0084 -0.3293 -0.0127 -0.1399  0.4624 -0.7610 -0.8696  0.1720 -0.0272\n",
              "\n",
              "[500000 rows x 562 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%time vX  = pd.read_csv('testX.csv', index_col='id')  # load testing input features X (only)\n",
        "%time tYX = pd.read_csv('trainYX.csv')                # partially load training labels Y and input features X\n",
        "tYX  # 561 input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "ATi5O9CIVek3",
        "outputId": "a930525a-e4e2-4fdc-a3e1-8088242a0002"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>83502</td>\n",
              "      <td>72554</td>\n",
              "      <td>66901</td>\n",
              "      <td>87427</td>\n",
              "      <td>93667</td>\n",
              "      <td>95949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       1      2      3      4      5      6\n",
              "y  83502  72554  66901  87427  93667  95949"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tYX.y.value_counts(sort=False).to_frame().T  # counts of observations in each label category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olesMyd1iF2x",
        "outputId": "df23453f-a654-49c8-a0b7-777ecf1aea0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ started. You have 60 sec. Good luck!\n"
          ]
        }
      ],
      "source": [
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_166qwXkhdN"
      },
      "source": [
        "<font size=5>⏳</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer Start Here...</font></strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snU9iOkJV79w"
      },
      "source": [
        "**TODO. Explain your preprocessing:** i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "egL202TMWGbi"
      },
      "outputs": [],
      "source": [
        "tX, tY = tYX.drop('y', axis=1).head(50000), tYX.head(50000).y-1               # shift labels by -1 to range {0,1,2,3,4,5}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2j4QCHJn96f"
      },
      "source": [
        "**TODO. Explain your modeling approach:** ideas you tried and why you thought they would be helpful. Takeaway: how these decisions guided you in modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a09r0FFzPTM1",
        "outputId": "47405dfd-a002-4339-c801-6a2d1e22d88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                5620      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,686\n",
            "Trainable params: 5,686\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(0)   # always seed your experiments\n",
        "Init = keras.initializers.RandomNormal(seed=0) # seed weights and biases\n",
        "\n",
        "m = keras.models.Sequential([\n",
        "    Dense(10, kernel_initializer=Init, input_shape=[tX.shape[1]]), # hidden layer with 100 neurons requires 561x100 trainable weights + 100 biases\n",
        "    Dense(6,  kernel_initializer=Init, activation='softmax')])     # one output neuron for each label. Softmax for multiclass (single label)\n",
        "m.summary()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()              # Maps 0-based integer labels to one-hot encodings\n",
        "m.compile(loss=loss, optimizer=\"sgd\", metrics=[\"accuracy\"])         # Accuracy is not really needed since it's equivalent to loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iflSI9xvfrWi",
        "outputId": "db8825c4-c52d-4980-a401-332adf2e238c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1094/1094 [==============================] - 3s 2ms/step - loss: 0.8193 - accuracy: 0.6982 - val_loss: 0.4222 - val_accuracy: 0.8655\n",
            "Epoch 2/3\n",
            "1094/1094 [==============================] - 2s 2ms/step - loss: 0.3162 - accuracy: 0.8944 - val_loss: 0.2434 - val_accuracy: 0.9184\n",
            "Epoch 3/3\n",
            "1094/1094 [==============================] - 2s 2ms/step - loss: 0.2059 - accuracy: 0.9285 - val_loss: 0.1732 - val_accuracy: 0.9363\n"
          ]
        }
      ],
      "source": [
        "hist = m.fit(tX, tY, epochs=3, validation_split=.3)  # validation loss is decreasing (as it should)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "PoAvjRciDjkR",
        "outputId": "43df955e-72f6-4167-a584-2800d8faccb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "History object contains: loss, accuracy, val_loss, val_accuracy\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.6.3.min.js\"></script>                <div id=\"5738b9db-e052-4bea-9f0f-877b55841bd2\" class=\"plotly-graph-div\" style=\"height:200px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5738b9db-e052-4bea-9f0f-877b55841bd2\")) {                    Plotly.newPlot(                        \"5738b9db-e052-4bea-9f0f-877b55841bd2\",                        [{\"hovertemplate\":\"epoch=%{x}<br>val_loss=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[1,2,3],\"xaxis\":\"x\",\"y\":[0.4221569001674652,0.24335601925849915,0.17323248088359833],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"val_loss\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":0,\"l\":0,\"r\":0,\"b\":0},\"height\":200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5738b9db-e052-4bea-9f0f-877b55841bd2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('History object contains: ' + ', '.join(hist.history.keys()))\n",
        "dfHist = pd.DataFrame(hist.history)\n",
        "dfHist['epoch'] = dfHist.index+1\n",
        "f = px.line(dfHist, x='epoch', y='val_loss', title='', markers=True);\n",
        "f = f.update_layout(height=200, margin=dict(l=0, r=0, t=0, b=0))\n",
        "f = f.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "dvf6AbnRsZU5",
        "outputId": "b4cf6b72-2e5b-4f32-caf9-3486a696c80d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_71367108_5609_11ec_aed5_0242ac1c0002row0_col0,#T_71367108_5609_11ec_aed5_0242ac1c0002row1_col5,#T_71367108_5609_11ec_aed5_0242ac1c0002row2_col5{\n",
              "            background-color:  #b40426;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_71367108_5609_11ec_aed5_0242ac1c0002row0_col1,#T_71367108_5609_11ec_aed5_0242ac1c0002row0_col2{\n",
              "            background-color:  #3e51c5;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_71367108_5609_11ec_aed5_0242ac1c0002row0_col3,#T_71367108_5609_11ec_aed5_0242ac1c0002row0_col4,#T_71367108_5609_11ec_aed5_0242ac1c0002row0_col5,#T_71367108_5609_11ec_aed5_0242ac1c0002row1_col0,#T_71367108_5609_11ec_aed5_0242ac1c0002row1_col1,#T_71367108_5609_11ec_aed5_0242ac1c0002row1_col2,#T_71367108_5609_11ec_aed5_0242ac1c0002row1_col4,#T_71367108_5609_11ec_aed5_0242ac1c0002row2_col0,#T_71367108_5609_11ec_aed5_0242ac1c0002row2_col1,#T_71367108_5609_11ec_aed5_0242ac1c0002row2_col2,#T_71367108_5609_11ec_aed5_0242ac1c0002row2_col4{\n",
              "            background-color:  #3b4cc0;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_71367108_5609_11ec_aed5_0242ac1c0002row1_col3{\n",
              "            background-color:  #3c4ec2;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_71367108_5609_11ec_aed5_0242ac1c0002row2_col3{\n",
              "            background-color:  #445acc;\n",
              "            color:  #f1f1f1;\n",
              "        }</style><table id=\"T_71367108_5609_11ec_aed5_0242ac1c0002\" class=\"dataframe\"><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0/walking</th>        <th class=\"col_heading level0 col1\" >1/walking_upstairs</th>        <th class=\"col_heading level0 col2\" >2/walking_downstairs</th>        <th class=\"col_heading level0 col3\" >3/sitting</th>        <th class=\"col_heading level0 col4\" >4/standing</th>        <th class=\"col_heading level0 col5\" >5/laying</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_71367108_5609_11ec_aed5_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.969090</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.014947</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.014097</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0.000201</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row0_col4\" class=\"data row0 col4\" >0.001664</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row0_col5\" class=\"data row0 col5\" >0.000002</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_71367108_5609_11ec_aed5_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0.004081</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row1_col4\" class=\"data row1 col4\" >0.000001</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row1_col5\" class=\"data row1 col5\" >0.995918</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_71367108_5609_11ec_aed5_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0.030873</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row2_col4\" class=\"data row2 col4\" >0.000001</td>\n",
              "                        <td id=\"T_71367108_5609_11ec_aed5_0242ac1c0002row2_col5\" class=\"data row2 col5\" >0.969126</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fc713105d50>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pOneHot = m.predict(vX)   # probabilities for each category. Subjects are rows\n",
        "YLab = [f'{i}/{s}' for i, s in enumerate('walking walking_upstairs walking_downstairs sitting standing laying'.split())]  # column labels\n",
        "pd.DataFrame(pOneHot[:3,:], columns=YLab).style.background_gradient(cmap='coolwarm', axis=1)  # display first few predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "lHyhHcGyktLC",
        "outputId": "6fcf6d2b-f8c4-41dd-80bb-1ec0f62c05af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>...</th>\n",
              "      <th>2897</th>\n",
              "      <th>2898</th>\n",
              "      <th>2899</th>\n",
              "      <th>2900</th>\n",
              "      <th>2901</th>\n",
              "      <th>2902</th>\n",
              "      <th>2903</th>\n",
              "      <th>2904</th>\n",
              "      <th>2905</th>\n",
              "      <th>2906</th>\n",
              "      <th>2907</th>\n",
              "      <th>2908</th>\n",
              "      <th>2909</th>\n",
              "      <th>2910</th>\n",
              "      <th>2911</th>\n",
              "      <th>2912</th>\n",
              "      <th>2913</th>\n",
              "      <th>2914</th>\n",
              "      <th>2915</th>\n",
              "      <th>2916</th>\n",
              "      <th>2917</th>\n",
              "      <th>2918</th>\n",
              "      <th>2919</th>\n",
              "      <th>2920</th>\n",
              "      <th>2921</th>\n",
              "      <th>2922</th>\n",
              "      <th>2923</th>\n",
              "      <th>2924</th>\n",
              "      <th>2925</th>\n",
              "      <th>2926</th>\n",
              "      <th>2927</th>\n",
              "      <th>2928</th>\n",
              "      <th>2929</th>\n",
              "      <th>2930</th>\n",
              "      <th>2931</th>\n",
              "      <th>2932</th>\n",
              "      <th>2933</th>\n",
              "      <th>2934</th>\n",
              "      <th>2935</th>\n",
              "      <th>2936</th>\n",
              "      <th>2937</th>\n",
              "      <th>2938</th>\n",
              "      <th>2939</th>\n",
              "      <th>2940</th>\n",
              "      <th>2941</th>\n",
              "      <th>2942</th>\n",
              "      <th>2943</th>\n",
              "      <th>2944</th>\n",
              "      <th>2945</th>\n",
              "      <th>2946</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 2947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8     9     10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29    30    31    32    33    34    35    36    37    38    39    40    41    42    43    44    45    46    47    48    49    ...  2897  2898  2899  2900  2901  2902  2903  2904  2905  2906  2907  2908  2909  2910  2911  2912  2913  2914  2915  2916  2917  2918  2919  2920  2921  2922  2923  2924  2925  2926  2927  2928  2929  2930  2931  2932  2933  2934  2935  2936  2937  2938  2939  2940  2941  2942  2943  2944  2945  2946\n",
              "y     1     6     6     2     1     6     2     6     1     5     6     3     6     5     1     5     3     2     2     2     2     3     4     2     5     1     4     5     5     5     3     3     2     3     3     2     5     5     6     5     3     5     6     6     6     2     2     1     6     3  ...     2     5     4     5     2     3     2     6     1     5     1     5     2     1     4     4     2     2     1     3     1     5     5     2     6     3     6     6     6     3     6     5     3     5     1     4     1     3     6     6     2     1     3     3     4     5     6     3     4     2\n",
              "\n",
              "[1 rows x 2947 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pY = pd.DataFrame(np.argmax(pOneHot, axis = 1)+1, columns=['y'])  # predicted labels (from 1 to 6)\n",
        "pY.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AEmcHGfFsobz"
      },
      "outputs": [],
      "source": [
        "ToCSV(pY, 'HAR_baseline')  # generate a CSV submission file for Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbty_ZankUul"
      },
      "source": [
        "<font size=5>⏳</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D4qqqpKjDsT",
        "outputId": "5e00108b-f045-4c8d-95c8-df16f7ecdbb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Runtime is 10 sec\n"
          ]
        }
      ],
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkiAK0ztkaI_"
      },
      "source": [
        "# 💡**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgh9fVRo-oA6"
      },
      "source": [
        "1. Try tuning DNN hyperparameters\n",
        "1. Training set has 500K observations (2GB), but you really don't need them all. They are all bootstrapped (with noise) from the original sample of 7352 observations. In order to stay within Colab runtime limit (CRTL), you can \n",
        "  1. use more observations for a shallow DNN, but risk underfitting due to lower model complexity\n",
        "  1. use fewer observations for a deeper DNN, but risk overfitting to higher model complexity\n",
        "1. Check out the original related papers about feature engineering for this dataset\n",
        "1. Try engineering features with [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) and discarding unimportant features via PCA or alternative technique.\n",
        "1. Consider KMeans/KMedoid or other clustering methods to identify observations, which represent the original 7352 observations. It might require finding 7352 cluster centroids/medoids.\n",
        "  1. Fast clustering methods: [FAISS](https://github.com/facebookresearch/faiss) (GPU-enabled)\n",
        "1. For deep NN, consider dropout, batch normalization\n",
        "1. Try PCA on transposed matrix to find/eliminate highly correlated observations\n",
        "1. Try [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) to ensure each label is proportionally represented in a subsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "HngvaaXwnVAE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ApglfImBXKKL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOrzvtfAXLdA",
        "outputId": "fb2c5f47-ef57-42af-f918-04e5d6a0f454"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500000, 562)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tYX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t7DVwhZXqaC",
        "outputId": "6cd206a3-1268-4a8b-ac07-2286850dc40c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2947, 561)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "OtWV4Z7qn7Ju"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "s2aoEoTSo9pi"
      },
      "outputs": [],
      "source": [
        "#tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "f3ACb0nIpCa6"
      },
      "outputs": [],
      "source": [
        "train_df = tYX\n",
        "test_df = vX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "hPN8Qy7cqoqi",
        "outputId": "1c35a809-6f0e-428d-fcef-cdb55699bbd5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>...</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "      <th>520</th>\n",
              "      <th>521</th>\n",
              "      <th>522</th>\n",
              "      <th>523</th>\n",
              "      <th>524</th>\n",
              "      <th>525</th>\n",
              "      <th>526</th>\n",
              "      <th>527</th>\n",
              "      <th>528</th>\n",
              "      <th>529</th>\n",
              "      <th>530</th>\n",
              "      <th>531</th>\n",
              "      <th>532</th>\n",
              "      <th>533</th>\n",
              "      <th>534</th>\n",
              "      <th>535</th>\n",
              "      <th>536</th>\n",
              "      <th>537</th>\n",
              "      <th>538</th>\n",
              "      <th>539</th>\n",
              "      <th>540</th>\n",
              "      <th>541</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2778</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0676</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9390</td>\n",
              "      <td>-0.4230</td>\n",
              "      <td>-0.7520</td>\n",
              "      <td>0.8496</td>\n",
              "      <td>0.6226</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-1.0370</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-1.0070</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-0.6772</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.5900</td>\n",
              "      <td>-0.3162</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.4440</td>\n",
              "      <td>-0.2622</td>\n",
              "      <td>0.1092</td>\n",
              "      <td>0.4468</td>\n",
              "      <td>-0.4443</td>\n",
              "      <td>-0.1484</td>\n",
              "      <td>0.1718</td>\n",
              "      <td>-0.2727</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>-0.4720</td>\n",
              "      <td>-0.5264</td>\n",
              "      <td>0.2332</td>\n",
              "      <td>0.9640</td>\n",
              "      <td>-0.1309</td>\n",
              "      <td>0.1071</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9126</td>\n",
              "      <td>-0.2037</td>\n",
              "      <td>-0.5300</td>\n",
              "      <td>-0.8164</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.8850</td>\n",
              "      <td>-0.9033</td>\n",
              "      <td>-0.9120</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-1.014</td>\n",
              "      <td>-0.9560</td>\n",
              "      <td>-0.6780</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.6180</td>\n",
              "      <td>-0.1021</td>\n",
              "      <td>-0.5977</td>\n",
              "      <td>-0.9546</td>\n",
              "      <td>-0.9110</td>\n",
              "      <td>-0.9260</td>\n",
              "      <td>-0.9297</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.9460</td>\n",
              "      <td>-1.022</td>\n",
              "      <td>-0.9570</td>\n",
              "      <td>-0.2930</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.3455</td>\n",
              "      <td>-0.1411</td>\n",
              "      <td>-0.5215</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9890</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.4453</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>-0.5415</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.5093</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>-0.0912</td>\n",
              "      <td>-0.1415</td>\n",
              "      <td>-0.1316</td>\n",
              "      <td>-0.8200</td>\n",
              "      <td>0.1721</td>\n",
              "      <td>-0.0535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>-0.1046</td>\n",
              "      <td>-0.2010</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>-0.2776</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>-0.2605</td>\n",
              "      <td>-0.0572</td>\n",
              "      <td>-0.0364</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.1448</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>-0.0844</td>\n",
              "      <td>-0.6733</td>\n",
              "      <td>-0.7603</td>\n",
              "      <td>-0.7847</td>\n",
              "      <td>-0.4136</td>\n",
              "      <td>-0.3633</td>\n",
              "      <td>-0.1837</td>\n",
              "      <td>0.2830</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>-0.2502</td>\n",
              "      <td>0.3079</td>\n",
              "      <td>-0.1384</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.1969</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>-0.3472</td>\n",
              "      <td>-0.1434</td>\n",
              "      <td>-0.4058</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.9326</td>\n",
              "      <td>-0.2942</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.9634</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8115</td>\n",
              "      <td>0.4165</td>\n",
              "      <td>-0.4731</td>\n",
              "      <td>-0.8210</td>\n",
              "      <td>0.2542</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.2688</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>-0.7710</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.1018</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>-0.8994</td>\n",
              "      <td>-0.0642</td>\n",
              "      <td>-0.0842</td>\n",
              "      <td>-0.4750</td>\n",
              "      <td>-0.1345</td>\n",
              "      <td>-0.3853</td>\n",
              "      <td>-0.2573</td>\n",
              "      <td>-0.5430</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.1365</td>\n",
              "      <td>-0.677</td>\n",
              "      <td>-0.1826</td>\n",
              "      <td>0.6777</td>\n",
              "      <td>-0.7866</td>\n",
              "      <td>0.3240</td>\n",
              "      <td>-0.6206</td>\n",
              "      <td>-0.8530</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.3025</td>\n",
              "      <td>-0.3176</td>\n",
              "      <td>-0.3198</td>\n",
              "      <td>-0.6426</td>\n",
              "      <td>-0.2488</td>\n",
              "      <td>-0.7236</td>\n",
              "      <td>-0.2512</td>\n",
              "      <td>0.6177</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>-0.0397</td>\n",
              "      <td>-0.4220</td>\n",
              "      <td>0.5480</td>\n",
              "      <td>0.6455</td>\n",
              "      <td>0.2296</td>\n",
              "      <td>-0.0335</td>\n",
              "      <td>-0.7000</td>\n",
              "      <td>0.2998</td>\n",
              "      <td>0.0880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>-0.0132</td>\n",
              "      <td>-0.1257</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9346</td>\n",
              "      <td>-0.5630</td>\n",
              "      <td>-0.8394</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6846</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0160</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.6500</td>\n",
              "      <td>-0.5225</td>\n",
              "      <td>-0.7974</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.2532</td>\n",
              "      <td>0.3723</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>-0.2756</td>\n",
              "      <td>0.3179</td>\n",
              "      <td>-0.1398</td>\n",
              "      <td>0.0948</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>-0.1853</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>-0.0402</td>\n",
              "      <td>-0.0880</td>\n",
              "      <td>0.9785</td>\n",
              "      <td>-0.0442</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.028</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9697</td>\n",
              "      <td>-1.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2356</td>\n",
              "      <td>0.4312</td>\n",
              "      <td>-0.6030</td>\n",
              "      <td>-0.8706</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>-1.0210</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>-0.4620</td>\n",
              "      <td>-0.7485</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-1.008</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.8574</td>\n",
              "      <td>-0.9210</td>\n",
              "      <td>0.1049</td>\n",
              "      <td>-0.6284</td>\n",
              "      <td>-0.8970</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0030</td>\n",
              "      <td>-0.844</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>-0.3782</td>\n",
              "      <td>-0.7183</td>\n",
              "      <td>-0.0227</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.1864</td>\n",
              "      <td>0.4556</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.0595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2695</td>\n",
              "      <td>-0.0251</td>\n",
              "      <td>-0.1010</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9050</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.8920</td>\n",
              "      <td>-0.9673</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.5293</td>\n",
              "      <td>-0.8022</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.6714</td>\n",
              "      <td>0.8480</td>\n",
              "      <td>-0.9624</td>\n",
              "      <td>-1.0205</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9600</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9480</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.7320</td>\n",
              "      <td>-0.5117</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0.3710</td>\n",
              "      <td>-0.2270</td>\n",
              "      <td>0.2700</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>-0.2438</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>-0.2886</td>\n",
              "      <td>-0.3882</td>\n",
              "      <td>0.6284</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>-0.1277</td>\n",
              "      <td>0.0722</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.925</td>\n",
              "      <td>-0.9440</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>0.0488</td>\n",
              "      <td>-0.3591</td>\n",
              "      <td>-0.7050</td>\n",
              "      <td>-1.0240</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.013</td>\n",
              "      <td>-0.9860</td>\n",
              "      <td>-0.9650</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.1430</td>\n",
              "      <td>-0.1555</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>-0.9320</td>\n",
              "      <td>-0.9200</td>\n",
              "      <td>-0.9424</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.932</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.985</td>\n",
              "      <td>-0.9463</td>\n",
              "      <td>-0.4020</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.3160</td>\n",
              "      <td>-0.0948</td>\n",
              "      <td>-0.4695</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>-0.9976</td>\n",
              "      <td>-0.9680</td>\n",
              "      <td>-1.0340</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.6980</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.4863</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>-0.3293</td>\n",
              "      <td>-0.0127</td>\n",
              "      <td>-0.1399</td>\n",
              "      <td>0.4624</td>\n",
              "      <td>-0.7610</td>\n",
              "      <td>-0.8696</td>\n",
              "      <td>0.1720</td>\n",
              "      <td>-0.0272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows × 562 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        y       0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43     44      45      46      47      48  ...     511     512     513     514     515     516     517     518     519     520    521     522     523     524     525     526     527     528     529     530     531    532     533    534     535     536     537     538     539     540     541     542     543     544     545     546     547     548     549    550     551     552     553     554     555     556     557     558     559     560\n",
              "0       5  0.2778  0.0092 -0.0676 -0.9785 -0.9160 -0.9610 -0.9834 -0.9170 -0.9590 -0.9390 -0.4230 -0.7520  0.8496  0.6226  0.8400 -0.9434 -0.9614 -1.0370 -1.0150 -1.0070 -0.9640 -0.9550 -0.6772  0.0568  0.0192  0.5900 -0.3162  0.1833  0.4440 -0.2622  0.1092  0.4468 -0.4443 -0.1484  0.1718 -0.2727  0.0954 -0.4720 -0.5264  0.2332  0.9640 -0.1309  0.1071 -0.9814 -0.948 -0.9727 -0.9720 -0.9575 -0.9585  ... -0.9126 -0.2037 -0.5300 -0.8164 -0.9170 -0.8850 -0.9033 -0.9120 -0.9750 -0.9326 -1.014 -0.9560 -0.6780 -0.9966 -0.6180 -0.1021 -0.5977 -0.9546 -0.9110 -0.9260 -0.9297 -1.017 -0.9460 -1.022 -0.9570 -0.2930 -1.0100 -0.3455 -0.1411 -0.5215 -0.9585 -0.9160 -0.9434 -0.9414 -0.9750 -0.9414 -0.9890 -0.9610 -0.4453 -1.002 -0.5415 -0.0308 -0.5093  0.0380 -0.0912 -0.1415 -0.1316 -0.8200  0.1721 -0.0535\n",
              "1       1  0.2454  0.0073 -0.1046 -0.2010  0.1426 -0.2668 -0.2776  0.0648 -0.2605 -0.0572 -0.0364 -0.2830 -0.2830 -0.1448  0.4443 -0.0844 -0.6733 -0.7603 -0.7847 -0.4136 -0.3633 -0.1837  0.2830  0.5100  0.0582 -0.2502  0.3079 -0.1384  0.0822  0.0902 -0.0034  0.1969  0.0538  0.2996 -0.0258  0.0936 -0.3472 -0.1434 -0.4058  0.3690  0.9326 -0.2942 -0.0916 -0.9966 -0.964 -0.9663 -0.9746 -0.9736 -0.9634  ... -0.8115  0.4165 -0.4731 -0.8210  0.2542  0.2410  0.2688  0.0928 -0.7710  0.2430 -0.221 -0.1018  0.7134 -0.8994 -0.0642 -0.0842 -0.4750 -0.1345 -0.3853 -0.2573 -0.5430 -0.757 -0.1365 -0.677 -0.1826  0.6777 -0.7866  0.3240 -0.6206 -0.8530 -0.2500 -0.3025 -0.3176 -0.3198 -0.6426 -0.2488 -0.7236 -0.2512  0.6177 -0.910  0.1069 -0.0397 -0.4220  0.5480  0.6455  0.2296 -0.0335 -0.7000  0.2998  0.0880\n",
              "...    ..     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
              "499998  4  0.2740 -0.0132 -0.1257 -0.9834 -1.0020 -0.9590 -0.9897 -0.9746 -0.9873 -0.9346 -0.5630 -0.8394  0.8306  0.6846  0.8350 -0.9840 -0.9824 -0.9960 -1.0200 -0.9950 -1.0160 -0.9590 -0.6500 -0.5225 -0.7974  0.5020 -0.2532  0.3723  0.1772  0.2920 -0.2756  0.3179 -0.1398  0.0948  0.0180 -0.1853  0.1871  0.0790 -0.0402 -0.0880  0.9785 -0.0442 -0.0532 -0.9950 -1.028 -0.9790 -1.0010 -0.9697 -1.0060  ... -0.2356  0.4312 -0.6030 -0.8706 -0.9700 -0.9863 -1.0010 -0.9990 -0.9927 -0.9910 -1.020 -1.0210 -0.9920 -0.9736  0.3857 -0.4620 -0.7485 -0.9985 -0.9575 -0.9897 -0.9814 -1.000 -0.9990 -1.008 -0.9660 -0.8574 -0.9210  0.1049 -0.6284 -0.8970 -1.0200 -1.0150 -0.9750 -1.0170 -0.9746 -0.9937 -0.9927 -0.9950 -1.0030 -0.844  0.2454 -0.3782 -0.7183 -0.0227  0.1957  0.1864  0.4556 -0.9326  0.1137  0.0595\n",
              "499999  5  0.2695 -0.0251 -0.1010 -1.0170 -0.9050 -0.9375 -0.9736 -0.8920 -0.9673 -0.9575 -0.5293 -0.8022  0.8530  0.6714  0.8480 -0.9624 -1.0205 -0.9900 -0.9600 -0.9960 -0.9480 -0.9720 -0.7320 -0.5117 -0.3535  0.3710 -0.2270  0.2700 -0.0636 -0.2438  0.0608  0.2050 -0.0218 -0.1199  0.0678  0.0154 -0.1132 -0.2886 -0.3882  0.6284  0.9966 -0.1277  0.0722 -1.0050 -0.925 -0.9440 -1.0050 -0.9824 -0.9233  ... -0.9500  0.0488 -0.3591 -0.7050 -1.0240 -0.9790 -0.9746 -0.9814 -0.9920 -0.9814 -1.013 -0.9860 -0.9650 -1.0150 -0.1430 -0.1555 -0.5180 -0.9320 -0.9200 -0.9424 -0.9326 -0.932 -0.9170 -0.985 -0.9463 -0.4020 -0.9640 -0.3160 -0.0948 -0.4695 -0.9590 -0.9500 -0.9976 -0.9680 -1.0340 -0.9727 -0.9900 -0.9790 -0.6980 -1.017 -0.4863  0.0084 -0.3293 -0.0127 -0.1399  0.4624 -0.7610 -0.8696  0.1720 -0.0272\n",
              "\n",
              "[500000 rows x 562 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "W5KCsT1Kqrqi",
        "outputId": "762c542c-d716-4409-9dd3-dd8b9d740aa0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>...</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "      <th>520</th>\n",
              "      <th>521</th>\n",
              "      <th>522</th>\n",
              "      <th>523</th>\n",
              "      <th>524</th>\n",
              "      <th>525</th>\n",
              "      <th>526</th>\n",
              "      <th>527</th>\n",
              "      <th>528</th>\n",
              "      <th>529</th>\n",
              "      <th>530</th>\n",
              "      <th>531</th>\n",
              "      <th>532</th>\n",
              "      <th>533</th>\n",
              "      <th>534</th>\n",
              "      <th>535</th>\n",
              "      <th>536</th>\n",
              "      <th>537</th>\n",
              "      <th>538</th>\n",
              "      <th>539</th>\n",
              "      <th>540</th>\n",
              "      <th>541</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.3142</td>\n",
              "      <td>-0.0251</td>\n",
              "      <td>-0.1250</td>\n",
              "      <td>-0.4185</td>\n",
              "      <td>-0.1707</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>-0.4497</td>\n",
              "      <td>-0.1836</td>\n",
              "      <td>-0.4100</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>-0.1823</td>\n",
              "      <td>-0.3806</td>\n",
              "      <td>0.3716</td>\n",
              "      <td>0.0620</td>\n",
              "      <td>0.5757</td>\n",
              "      <td>-0.3030</td>\n",
              "      <td>-0.8370</td>\n",
              "      <td>-0.8640</td>\n",
              "      <td>-0.8535</td>\n",
              "      <td>-0.5205</td>\n",
              "      <td>-0.3486</td>\n",
              "      <td>-0.3962</td>\n",
              "      <td>0.3965</td>\n",
              "      <td>0.3147</td>\n",
              "      <td>0.0732</td>\n",
              "      <td>-0.3600</td>\n",
              "      <td>0.4092</td>\n",
              "      <td>-0.2252</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>-0.0731</td>\n",
              "      <td>0.2054</td>\n",
              "      <td>-0.0099</td>\n",
              "      <td>-0.0017</td>\n",
              "      <td>-0.1094</td>\n",
              "      <td>0.1564</td>\n",
              "      <td>-0.0598</td>\n",
              "      <td>-0.0450</td>\n",
              "      <td>-0.3875</td>\n",
              "      <td>-0.0948</td>\n",
              "      <td>0.2620</td>\n",
              "      <td>0.9100</td>\n",
              "      <td>-0.3389</td>\n",
              "      <td>0.0237</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.970</td>\n",
              "      <td>-0.9175</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>0.8623</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.7340</td>\n",
              "      <td>0.3370</td>\n",
              "      <td>-0.6990</td>\n",
              "      <td>-0.8880</td>\n",
              "      <td>-0.2041</td>\n",
              "      <td>-0.2680</td>\n",
              "      <td>-0.1696</td>\n",
              "      <td>-0.3704</td>\n",
              "      <td>-0.5435</td>\n",
              "      <td>-0.2032</td>\n",
              "      <td>-0.6910</td>\n",
              "      <td>-0.2400</td>\n",
              "      <td>0.3840</td>\n",
              "      <td>-0.8750</td>\n",
              "      <td>0.2517</td>\n",
              "      <td>-0.1758</td>\n",
              "      <td>-0.5566</td>\n",
              "      <td>-0.5117</td>\n",
              "      <td>-0.6360</td>\n",
              "      <td>-0.5130</td>\n",
              "      <td>-0.7744</td>\n",
              "      <td>-0.7880</td>\n",
              "      <td>-0.5244</td>\n",
              "      <td>-0.8810</td>\n",
              "      <td>-0.4631</td>\n",
              "      <td>0.4960</td>\n",
              "      <td>-0.3840</td>\n",
              "      <td>0.1997</td>\n",
              "      <td>-0.7500</td>\n",
              "      <td>-0.9570</td>\n",
              "      <td>-0.6440</td>\n",
              "      <td>-0.6006</td>\n",
              "      <td>-0.5713</td>\n",
              "      <td>-0.6700</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.659</td>\n",
              "      <td>-0.9023</td>\n",
              "      <td>-0.5880</td>\n",
              "      <td>0.3484</td>\n",
              "      <td>-0.8745</td>\n",
              "      <td>0.0124</td>\n",
              "      <td>-0.2009</td>\n",
              "      <td>-0.6180</td>\n",
              "      <td>-0.5780</td>\n",
              "      <td>0.7380</td>\n",
              "      <td>-0.9350</td>\n",
              "      <td>-0.2076</td>\n",
              "      <td>-0.6700</td>\n",
              "      <td>0.3171</td>\n",
              "      <td>0.0052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.2496</td>\n",
              "      <td>-0.0210</td>\n",
              "      <td>-0.0990</td>\n",
              "      <td>-0.9480</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9120</td>\n",
              "      <td>-0.5280</td>\n",
              "      <td>-0.8010</td>\n",
              "      <td>0.8066</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-1.0110</td>\n",
              "      <td>-0.9410</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.3086</td>\n",
              "      <td>-0.5776</td>\n",
              "      <td>-0.3179</td>\n",
              "      <td>-0.1814</td>\n",
              "      <td>0.0620</td>\n",
              "      <td>0.1808</td>\n",
              "      <td>-0.3193</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>-0.0446</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.4705</td>\n",
              "      <td>0.1150</td>\n",
              "      <td>-0.0302</td>\n",
              "      <td>0.1721</td>\n",
              "      <td>-0.1791</td>\n",
              "      <td>0.0714</td>\n",
              "      <td>-0.3232</td>\n",
              "      <td>-0.5470</td>\n",
              "      <td>-0.4750</td>\n",
              "      <td>0.6484</td>\n",
              "      <td>0.7666</td>\n",
              "      <td>-0.920</td>\n",
              "      <td>-0.9850</td>\n",
              "      <td>-0.980</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.5090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8620</td>\n",
              "      <td>-0.0361</td>\n",
              "      <td>-0.6943</td>\n",
              "      <td>-0.9287</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9697</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.8945</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>0.3616</td>\n",
              "      <td>-0.2722</td>\n",
              "      <td>-0.6470</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-0.9634</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.4648</td>\n",
              "      <td>-0.9000</td>\n",
              "      <td>0.1111</td>\n",
              "      <td>-0.5547</td>\n",
              "      <td>-0.8370</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9688</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.9624</td>\n",
              "      <td>-0.979</td>\n",
              "      <td>-1.0130</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.6577</td>\n",
              "      <td>-1.0130</td>\n",
              "      <td>0.3090</td>\n",
              "      <td>-0.0421</td>\n",
              "      <td>-0.4000</td>\n",
              "      <td>-0.0448</td>\n",
              "      <td>0.1063</td>\n",
              "      <td>0.3752</td>\n",
              "      <td>0.2942</td>\n",
              "      <td>0.5903</td>\n",
              "      <td>-0.4210</td>\n",
              "      <td>-0.5990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2945</th>\n",
              "      <td>0.2766</td>\n",
              "      <td>-0.0196</td>\n",
              "      <td>-0.1057</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9400</td>\n",
              "      <td>-0.5780</td>\n",
              "      <td>-0.8240</td>\n",
              "      <td>0.8570</td>\n",
              "      <td>0.6865</td>\n",
              "      <td>0.8440</td>\n",
              "      <td>-0.9880</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-1.0130</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9810</td>\n",
              "      <td>-0.5854</td>\n",
              "      <td>-0.7040</td>\n",
              "      <td>-0.5510</td>\n",
              "      <td>0.0740</td>\n",
              "      <td>-0.0154</td>\n",
              "      <td>0.0144</td>\n",
              "      <td>0.1282</td>\n",
              "      <td>0.0236</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.1385</td>\n",
              "      <td>-0.0140</td>\n",
              "      <td>-0.0650</td>\n",
              "      <td>0.1234</td>\n",
              "      <td>-0.1458</td>\n",
              "      <td>-0.2747</td>\n",
              "      <td>-0.2598</td>\n",
              "      <td>0.8945</td>\n",
              "      <td>0.2632</td>\n",
              "      <td>0.1770</td>\n",
              "      <td>-0.992</td>\n",
              "      <td>-1.0140</td>\n",
              "      <td>-1.008</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>0.8200</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.2798</td>\n",
              "      <td>-0.6830</td>\n",
              "      <td>-0.9175</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.0140</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.8086</td>\n",
              "      <td>0.3782</td>\n",
              "      <td>-0.8200</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9946</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-0.9233</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>0.2832</td>\n",
              "      <td>-0.5150</td>\n",
              "      <td>-0.7944</td>\n",
              "      <td>-0.9880</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.999</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-0.9520</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>0.2181</td>\n",
              "      <td>-0.3127</td>\n",
              "      <td>-0.5960</td>\n",
              "      <td>-0.1061</td>\n",
              "      <td>0.0445</td>\n",
              "      <td>-0.2370</td>\n",
              "      <td>-0.1945</td>\n",
              "      <td>-0.6606</td>\n",
              "      <td>-0.0980</td>\n",
              "      <td>-0.1029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2946</th>\n",
              "      <td>0.2560</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>-0.1188</td>\n",
              "      <td>-0.2546</td>\n",
              "      <td>-0.1289</td>\n",
              "      <td>-0.2465</td>\n",
              "      <td>-0.3528</td>\n",
              "      <td>-0.1543</td>\n",
              "      <td>-0.3027</td>\n",
              "      <td>-0.0241</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>-0.4731</td>\n",
              "      <td>0.2537</td>\n",
              "      <td>0.3340</td>\n",
              "      <td>0.2272</td>\n",
              "      <td>-0.2062</td>\n",
              "      <td>-0.7150</td>\n",
              "      <td>-0.8550</td>\n",
              "      <td>-0.7275</td>\n",
              "      <td>-0.5460</td>\n",
              "      <td>-0.4172</td>\n",
              "      <td>-0.4707</td>\n",
              "      <td>0.2406</td>\n",
              "      <td>0.2686</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>-0.2815</td>\n",
              "      <td>-0.0537</td>\n",
              "      <td>0.4592</td>\n",
              "      <td>-0.2090</td>\n",
              "      <td>-0.2181</td>\n",
              "      <td>0.1104</td>\n",
              "      <td>0.0714</td>\n",
              "      <td>0.0626</td>\n",
              "      <td>-0.4785</td>\n",
              "      <td>0.4792</td>\n",
              "      <td>-0.4140</td>\n",
              "      <td>0.1278</td>\n",
              "      <td>-0.2426</td>\n",
              "      <td>-0.4104</td>\n",
              "      <td>0.0434</td>\n",
              "      <td>0.9150</td>\n",
              "      <td>-0.1970</td>\n",
              "      <td>-0.2308</td>\n",
              "      <td>-0.957</td>\n",
              "      <td>-0.9000</td>\n",
              "      <td>-0.905</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.8926</td>\n",
              "      <td>-0.9200</td>\n",
              "      <td>0.8706</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5225</td>\n",
              "      <td>0.0452</td>\n",
              "      <td>-0.1909</td>\n",
              "      <td>-0.5044</td>\n",
              "      <td>-0.4340</td>\n",
              "      <td>-0.3894</td>\n",
              "      <td>-0.4300</td>\n",
              "      <td>-0.3887</td>\n",
              "      <td>-0.5474</td>\n",
              "      <td>-0.4275</td>\n",
              "      <td>-0.8174</td>\n",
              "      <td>-0.5810</td>\n",
              "      <td>0.1562</td>\n",
              "      <td>-0.8936</td>\n",
              "      <td>0.1070</td>\n",
              "      <td>0.2793</td>\n",
              "      <td>-0.0115</td>\n",
              "      <td>-0.5967</td>\n",
              "      <td>-0.5615</td>\n",
              "      <td>-0.5350</td>\n",
              "      <td>-0.6210</td>\n",
              "      <td>-0.8120</td>\n",
              "      <td>-0.5957</td>\n",
              "      <td>-0.9033</td>\n",
              "      <td>-0.6140</td>\n",
              "      <td>0.3967</td>\n",
              "      <td>-0.8480</td>\n",
              "      <td>-0.0174</td>\n",
              "      <td>-0.3145</td>\n",
              "      <td>-0.6787</td>\n",
              "      <td>-0.6510</td>\n",
              "      <td>-0.6353</td>\n",
              "      <td>-0.6704</td>\n",
              "      <td>-0.5986</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.651</td>\n",
              "      <td>-0.9336</td>\n",
              "      <td>-0.6740</td>\n",
              "      <td>0.2438</td>\n",
              "      <td>-0.9070</td>\n",
              "      <td>0.2262</td>\n",
              "      <td>0.3040</td>\n",
              "      <td>0.0236</td>\n",
              "      <td>0.2642</td>\n",
              "      <td>0.7856</td>\n",
              "      <td>0.9546</td>\n",
              "      <td>-0.4792</td>\n",
              "      <td>-0.7026</td>\n",
              "      <td>0.2227</td>\n",
              "      <td>0.1810</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2947 rows × 561 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42     43      44     45      46      47      48      49  ...     511     512     513     514     515     516     517     518     519     520     521     522     523     524     525     526     527     528     529     530     531     532     533     534     535     536     537     538     539     540     541     542     543     544     545    546     547     548     549     550     551     552     553     554     555     556     557     558     559     560\n",
              "id                                                                                                                                                                                                                                                                                                                                                                                                                  ...                                                                                                                                                                                                                                                                                                                                                                                                               \n",
              "0     0.3142 -0.0251 -0.1250 -0.4185 -0.1707 -0.4265 -0.4497 -0.1836 -0.4100 -0.1537 -0.1823 -0.3806  0.3716  0.0620  0.5757 -0.3030 -0.8370 -0.8640 -0.8535 -0.5205 -0.3486 -0.3962  0.3965  0.3147  0.0732 -0.3600  0.4092 -0.2252  0.0484 -0.0731  0.2054 -0.0099 -0.0017 -0.1094  0.1564 -0.0598 -0.0450 -0.3875 -0.0948  0.2620  0.9100 -0.3389  0.0237 -0.952 -0.9785 -0.970 -0.9175 -0.9910 -0.9575  0.8623  ... -0.7340  0.3370 -0.6990 -0.8880 -0.2041 -0.2680 -0.1696 -0.3704 -0.5435 -0.2032 -0.6910 -0.2400  0.3840 -0.8750  0.2517 -0.1758 -0.5566 -0.5117 -0.6360 -0.5130 -0.7744 -0.7880 -0.5244 -0.8810 -0.4631  0.4960 -0.3840  0.1997 -0.7500 -0.9570 -0.6440 -0.6006 -0.5713 -0.6700 -0.9610 -0.659 -0.9023 -0.5880  0.3484 -0.8745  0.0124 -0.2009 -0.6180 -0.5780  0.7380 -0.9350 -0.2076 -0.6700  0.3171  0.0052\n",
              "1     0.2496 -0.0210 -0.0990 -0.9480 -0.9590 -0.9790 -0.9614 -0.9660 -0.9840 -0.9120 -0.5280 -0.8010  0.8066  0.6685  0.8306 -0.9500 -0.9873 -0.9985 -1.0110 -0.9410 -0.9727 -0.9727 -0.3086 -0.5776 -0.3179 -0.1814  0.0620  0.1808 -0.3193  0.0590 -0.0446  0.0230  0.4705  0.1150 -0.0302  0.1721 -0.1791  0.0714 -0.3232 -0.5470 -0.4750  0.6484  0.7666 -0.920 -0.9850 -0.980 -0.9160 -0.9920 -0.9785 -0.5090  ... -0.8620 -0.0361 -0.6943 -0.9287 -0.9790 -0.9660 -0.9640 -0.9824 -0.9663 -0.9697 -1.0050 -0.9663 -0.8945 -0.9920  0.3616 -0.2722 -0.6470 -0.9663 -0.9614 -0.9634 -0.9610 -0.9840 -0.9863 -0.9985 -0.9700 -0.4648 -0.9000  0.1111 -0.5547 -0.8370 -0.9897 -0.9688 -0.9700 -0.9720 -0.9624 -0.979 -1.0130 -0.9840 -0.6577 -1.0130  0.3090 -0.0421 -0.4000 -0.0448  0.1063  0.3752  0.2942  0.5903 -0.4210 -0.5990\n",
              "...      ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...    ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
              "2945  0.2766 -0.0196 -0.1057 -0.9863 -0.9927 -0.9814 -1.0100 -0.9873 -0.9873 -0.9400 -0.5780 -0.8240  0.8570  0.6865  0.8440 -0.9880 -0.9985 -1.0130 -1.0010 -0.9927 -0.9897 -0.9810 -0.5854 -0.7040 -0.5510  0.0740 -0.0154  0.0144  0.1282  0.0236  0.0232  0.1582  0.1470  0.1385 -0.0140 -0.0650  0.1234 -0.1458 -0.2747 -0.2598  0.8945  0.2632  0.1770 -0.992 -1.0140 -1.008 -0.9834 -0.9873 -0.9990  0.8200  ... -1.0120  0.2798 -0.6830 -0.9175 -0.9960 -0.9814 -1.0140 -1.0150 -1.0050 -0.9937 -0.9840 -0.9920 -0.9985 -0.8086  0.3782 -0.8200 -0.9834 -1.0020 -0.9966 -1.0050 -1.0020 -0.9946 -0.9927 -0.9960 -0.9990 -0.9233 -0.9873  0.2832 -0.5150 -0.7944 -0.9880 -0.9937 -0.9824 -0.9910 -0.9960 -0.999 -1.0020 -0.9863 -0.9520 -0.9966  0.2181 -0.3127 -0.5960 -0.1061  0.0445 -0.2370 -0.1945 -0.6606 -0.0980 -0.1029\n",
              "2946  0.2560  0.0159 -0.1188 -0.2546 -0.1289 -0.2465 -0.3528 -0.1543 -0.3027 -0.0241  0.0451 -0.4731  0.2537  0.3340  0.2272 -0.2062 -0.7150 -0.8550 -0.7275 -0.5460 -0.4172 -0.4707  0.2406  0.2686  0.2996 -0.2815 -0.0537  0.4592 -0.2090 -0.2181  0.1104  0.0714  0.0626 -0.4785  0.4792 -0.4140  0.1278 -0.2426 -0.4104  0.0434  0.9150 -0.1970 -0.2308 -0.957 -0.9000 -0.905 -0.9326 -0.8926 -0.9200  0.8706  ... -0.5225  0.0452 -0.1909 -0.5044 -0.4340 -0.3894 -0.4300 -0.3887 -0.5474 -0.4275 -0.8174 -0.5810  0.1562 -0.8936  0.1070  0.2793 -0.0115 -0.5967 -0.5615 -0.5350 -0.6210 -0.8120 -0.5957 -0.9033 -0.6140  0.3967 -0.8480 -0.0174 -0.3145 -0.6787 -0.6510 -0.6353 -0.6704 -0.5986 -0.9326 -0.651 -0.9336 -0.6740  0.2438 -0.9070  0.2262  0.3040  0.0236  0.2642  0.7856  0.9546 -0.4792 -0.7026  0.2227  0.1810\n",
              "\n",
              "[2947 rows x 561 columns]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrRJsVVRpbdd",
        "outputId": "2e2b30fa-0c28-43d0-c842-c37fee9ddd18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Columns: 562 entries, y to 560\n",
            "dtypes: float64(561), int64(1)\n",
            "memory usage: 2.1 GB\n"
          ]
        }
      ],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68vEL3F2pvCF",
        "outputId": "e403d012-e20c-41a4-d53c-c49be2b1d6e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set missing values: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"Train set missing values:\", train_df.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd0OTPWzpy3g",
        "outputId": "d1889310-eb02-4110-f6a5-0730c3cfb4ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2947 entries, 0 to 2946\n",
            "Columns: 561 entries, 0 to 560\n",
            "dtypes: float64(561)\n",
            "memory usage: 12.6 MB\n"
          ]
        }
      ],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pbngaW9p_Er",
        "outputId": "0a030bbf-e00c-4b75-b283-da86b34d564a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set missing values: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"Test set missing values:\", test_df.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "gzvn5fxyRsqe"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = train_df.drop('y', axis=1).head(50000), train_df.head(50000).y-1   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERAp39LOqcLP",
        "outputId": "6b2a7661-7441-4719-d891-8ecc11593e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 10)                5620      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 6)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,686\n",
            "Trainable params: 5,686\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(0)   \n",
        "Init = keras.initializers.RandomNormal(seed=0) \n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    Dense(10, kernel_initializer=Init, input_shape=[tX.shape[1]]),\n",
        "    Dense(6,  kernel_initializer=Init, activation='softmax')])     \n",
        "model.summary()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()              \n",
        "model.compile(loss=loss, optimizer=\"adam\", metrics=[\"accuracy\"])         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAUgIIswrxrQ",
        "outputId": "13bbf5f5-6f32-4f67-e7fc-e2fdea84365a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 10)                5620      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 6)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,686\n",
            "Trainable params: 5,686\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH7Ybc0pr3SF",
        "outputId": "1d1e99ef-9c8f-44cf-a55e-c192cf41f1d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "70/70 [==============================] - 1s 9ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 0.0312 - val_accuracy: 0.9899 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0321 - accuracy: 0.9892 - val_loss: 0.0309 - val_accuracy: 0.9897 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0296 - accuracy: 0.9902 - val_loss: 0.0315 - val_accuracy: 0.9884 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "70/70 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 0.9909 - val_loss: 0.0307 - val_accuracy: 0.9896 - lr: 1.0000e-04\n",
            "Epoch 5/10\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9910 - val_loss: 0.0305 - val_accuracy: 0.9897 - lr: 1.0000e-04\n",
            "Epoch 6/10\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9909 - val_loss: 0.0304 - val_accuracy: 0.9897 - lr: 1.0000e-04\n",
            "Epoch 7/10\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.9911 - val_loss: 0.0305 - val_accuracy: 0.9897 - lr: 1.0000e-05\n",
            "Epoch 8/10\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.0304 - val_accuracy: 0.9897 - lr: 1.0000e-06\n",
            "Epoch 9/10\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.0304 - val_accuracy: 0.9897 - lr: 1.0000e-07\n",
            "Epoch 10/10\n",
            "70/70 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.0304 - val_accuracy: 0.9897 - lr: 1.0000e-08\n"
          ]
        }
      ],
      "source": [
        "batch_size = 500\n",
        "epochs = 10\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_split=0.3,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.1,\n",
        "            patience=1\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "2yF8Lj-BsKBN",
        "outputId": "c29e19a2-068d-4375-a1db-6f9174c50d47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1152x720 with 0 Axes>"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc70c933610>]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc70c933c90>]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc70c933b90>"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Over Time')"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.plot(\n",
        "    epochs_range,\n",
        "    train_loss,\n",
        "    label=\"Training Loss\"\n",
        ")\n",
        "\n",
        "plt.plot(\n",
        "    epochs_range,\n",
        "    val_loss,\n",
        "    label=\"Validation Loss\"\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Loss Over Time\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "a6IgNhZPJfPJ",
        "outputId": "02fdda30-ec8e-40d3-a5ee-283e9374d4f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_09641808_560b_11ec_aed5_0242ac1c0002row0_col0,#T_09641808_560b_11ec_aed5_0242ac1c0002row1_col5,#T_09641808_560b_11ec_aed5_0242ac1c0002row2_col5{\n",
              "            background-color:  #b40426;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_09641808_560b_11ec_aed5_0242ac1c0002row0_col1,#T_09641808_560b_11ec_aed5_0242ac1c0002row0_col2,#T_09641808_560b_11ec_aed5_0242ac1c0002row0_col3,#T_09641808_560b_11ec_aed5_0242ac1c0002row0_col4,#T_09641808_560b_11ec_aed5_0242ac1c0002row0_col5,#T_09641808_560b_11ec_aed5_0242ac1c0002row1_col0,#T_09641808_560b_11ec_aed5_0242ac1c0002row1_col1,#T_09641808_560b_11ec_aed5_0242ac1c0002row1_col2,#T_09641808_560b_11ec_aed5_0242ac1c0002row1_col3,#T_09641808_560b_11ec_aed5_0242ac1c0002row1_col4,#T_09641808_560b_11ec_aed5_0242ac1c0002row2_col0,#T_09641808_560b_11ec_aed5_0242ac1c0002row2_col1,#T_09641808_560b_11ec_aed5_0242ac1c0002row2_col2,#T_09641808_560b_11ec_aed5_0242ac1c0002row2_col3,#T_09641808_560b_11ec_aed5_0242ac1c0002row2_col4{\n",
              "            background-color:  #3b4cc0;\n",
              "            color:  #f1f1f1;\n",
              "        }</style><table id=\"T_09641808_560b_11ec_aed5_0242ac1c0002\" class=\"dataframe\"><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0/walking</th>        <th class=\"col_heading level0 col1\" >1/walking_upstairs</th>        <th class=\"col_heading level0 col2\" >2/walking_downstairs</th>        <th class=\"col_heading level0 col3\" >3/sitting</th>        <th class=\"col_heading level0 col4\" >4/standing</th>        <th class=\"col_heading level0 col5\" >5/laying</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_09641808_560b_11ec_aed5_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.999977</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.000007</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.000016</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row0_col4\" class=\"data row0 col4\" >0.000001</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_09641808_560b_11ec_aed5_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row1_col5\" class=\"data row1 col5\" >1.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_09641808_560b_11ec_aed5_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0.000004</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "                        <td id=\"T_09641808_560b_11ec_aed5_0242ac1c0002row2_col5\" class=\"data row2 col5\" >0.999996</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fc70c8f3c90>"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pOneHot = model.predict(test_df)   # probabilities for each category. Subjects are rows\n",
        "YLab = [f'{i}/{s}' for i, s in enumerate('walking walking_upstairs walking_downstairs sitting standing laying'.split())]  # column labels\n",
        "pd.DataFrame(pOneHot[:3,:], columns=YLab).style.background_gradient(cmap='coolwarm', axis=1)  # display first few predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "Od4MBuVbJnCI",
        "outputId": "55e143d0-59f7-43e6-9a42-8f155027127a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>...</th>\n",
              "      <th>2897</th>\n",
              "      <th>2898</th>\n",
              "      <th>2899</th>\n",
              "      <th>2900</th>\n",
              "      <th>2901</th>\n",
              "      <th>2902</th>\n",
              "      <th>2903</th>\n",
              "      <th>2904</th>\n",
              "      <th>2905</th>\n",
              "      <th>2906</th>\n",
              "      <th>2907</th>\n",
              "      <th>2908</th>\n",
              "      <th>2909</th>\n",
              "      <th>2910</th>\n",
              "      <th>2911</th>\n",
              "      <th>2912</th>\n",
              "      <th>2913</th>\n",
              "      <th>2914</th>\n",
              "      <th>2915</th>\n",
              "      <th>2916</th>\n",
              "      <th>2917</th>\n",
              "      <th>2918</th>\n",
              "      <th>2919</th>\n",
              "      <th>2920</th>\n",
              "      <th>2921</th>\n",
              "      <th>2922</th>\n",
              "      <th>2923</th>\n",
              "      <th>2924</th>\n",
              "      <th>2925</th>\n",
              "      <th>2926</th>\n",
              "      <th>2927</th>\n",
              "      <th>2928</th>\n",
              "      <th>2929</th>\n",
              "      <th>2930</th>\n",
              "      <th>2931</th>\n",
              "      <th>2932</th>\n",
              "      <th>2933</th>\n",
              "      <th>2934</th>\n",
              "      <th>2935</th>\n",
              "      <th>2936</th>\n",
              "      <th>2937</th>\n",
              "      <th>2938</th>\n",
              "      <th>2939</th>\n",
              "      <th>2940</th>\n",
              "      <th>2941</th>\n",
              "      <th>2942</th>\n",
              "      <th>2943</th>\n",
              "      <th>2944</th>\n",
              "      <th>2945</th>\n",
              "      <th>2946</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 2947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8     9     10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29    30    31    32    33    34    35    36    37    38    39    40    41    42    43    44    45    46    47    48    49    ...  2897  2898  2899  2900  2901  2902  2903  2904  2905  2906  2907  2908  2909  2910  2911  2912  2913  2914  2915  2916  2917  2918  2919  2920  2921  2922  2923  2924  2925  2926  2927  2928  2929  2930  2931  2932  2933  2934  2935  2936  2937  2938  2939  2940  2941  2942  2943  2944  2945  2946\n",
              "y     1     6     6     2     1     6     2     6     1     5     6     3     6     5     1     5     3     2     2     2     2     3     4     2     5     1     4     5     5     5     3     3     2     3     3     2     5     5     6     5     3     5     6     5     6     1     2     1     6     3  ...     2     5     4     5     2     3     2     6     1     5     1     5     2     3     4     5     2     2     1     3     1     5     5     2     6     3     6     6     6     3     6     5     3     5     1     4     1     3     6     6     2     1     3     2     4     5     6     3     4     2\n",
              "\n",
              "[1 rows x 2947 columns]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pY = pd.DataFrame(np.argmax(pOneHot, axis = 1)+1, columns=['y'])  # predicted labels (from 1 to 6)\n",
        "pY.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3PNheBpGJtL4"
      },
      "outputs": [],
      "source": [
        "ToCSV(pY, 'HAR_baseline')  # generate a CSV submission file for Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1n8bLecaObNx",
        "outputId": "1c9f7f8c-88c0-4d3b-c1b4-e74f26dd293a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_39ca4eb5-7a96-4ec8-9266-bc45ca432ae9\", \"HAR_baseline.csv\", 19524)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('HAR_baseline.csv') "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "\"HW11-Nov.22-DNN.Kaggle-HAR-[Calmanovici, Gilmutdinov, Malchenko].ipynb\"",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
